{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports all data in the data file into a list of lists. Each internal list contains the information of a full column. Empty fields are preserved in the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from pprint import pprint\n",
    "\n",
    "data = [[] for i in range(7)] \n",
    "\n",
    "with open('twcs.csv', 'r',encoding='utf-8') as infile:\n",
    "    datafile = csv.DictReader(infile)\n",
    "    for row in datafile:\n",
    "        i = 0\n",
    "        for item in row:\n",
    "            data[i].append(row[item])\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: The code below prints the length of the \"tweet_id\" column. Its result is the number of tweets in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2811774 Tweets in this file.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",len(data[0]),\"Tweets in this file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some tweet_id numbers do not occur within the file; for example, there is no tweet 7,9, or 10 upon initial inspection as shown below. In addition, some tweet information spans two lines in the file, such as tweet_id 34. This doesn't cause issue with analysis, but there are fewer tweets than lines in the file, which is worth noting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "11\n",
      "12\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for item in data[0][0:10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Some of the tweets contain non-english characters as shown below. This is actually the slowest part of the program. In hind sight, I could have done this differently, but it gets the job done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@115770 こんにちは、アマゾン公式です。Fire TV Stickが見れないというのは、どのような状況でしょうか。一般的なトラブルシューティングを記載したヘルプがございますので、ご参照ください。https://t.co/2pbG55qJ7h ET\n"
     ]
    }
   ],
   "source": [
    "print(data[4][data[0].index('269')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Here I determine the fraction of each tweet that is non-ASCII encoded, then count the number of tweets that consist of 50% or more non-ASCII encoded characters. I used this method of testing character ordinance because the original method I tried was too slow. The two methods gave slightly different results, though I understand this approach more thoroughly. My original attempt included the following code that was far too inefficient. What follows is much faster.\n",
    "\n",
    "        try:  #Snippet of initial attempt\n",
    "           character.encode('ascii')\n",
    "            totalChar+=1\n",
    "        except UnicodeEncodeError:\n",
    "            nonAscii+=1\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20245 Tweets containing 50% or more non-ASCII characters.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for leString in data[4]:\n",
    "    totalChar = 0\n",
    "    nonAscii = 0\n",
    "    for character in leString:\n",
    "        totalChar+=1\n",
    "        if ord(character) > 127:\n",
    "            nonAscii+=1\n",
    "    if totalChar == 0:\n",
    "        pass\n",
    "    else:\n",
    "        if nonAscii/totalChar >= 0.5:\n",
    "            count += 1\n",
    "            \n",
    "print(\"There are\",count,\"Tweets containing 50% or more non-ASCII characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Here I count the number of unique twitter names used by checking for an @ symbol at the start of each word, checking that against a running list of names, and adding it to the list if it doesn't exist or increasing a counter if it does. The while loop strips punctuation or other special characters off the end of each name, if applicable, and can handle any number of special characters provided they are not intertwined with alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 717692 unique Twitter names in this file\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "nameCounts = {}\n",
    "for leString in data[4]:\n",
    "    splitString = leString.split()\n",
    "    for word in splitString:\n",
    "        if word[0] == '@' and len(word) > 1:\n",
    "            while word[-1].isalnum() is False and word[-1] is not '_' and len(word)>1:\n",
    "                word = word[:-1]\n",
    "            if word in nameCounts:\n",
    "                nameCounts[word]+=1\n",
    "            elif len(word) > 1:\n",
    "                nameCounts.update({word:1})\n",
    "            \n",
    "            finalCount = len(nameCounts)\n",
    "        \n",
    "print(\"There are\",finalCount,\"unique Twitter names in this file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Using exactly the same code as Q4, I simply need to put the dictionary into a sorted list, then output the last 10 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following list is the top 10 Twitter usernames mentioned:\n",
      "@AmazonHelp\n",
      "@AppleSupport\n",
      "@AmericanAir\n",
      "@Uber_Support\n",
      "@Delta\n",
      "@115858\n",
      "@VirginTrains\n",
      "@Tesco\n",
      "@SouthwestAir\n",
      "@SpotifyCares\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "nameCounts = {}\n",
    "for leString in data[4]:\n",
    "    splitString = leString.split()\n",
    "    for word in splitString:\n",
    "        if word[0] == '@' and len(word) > 1:\n",
    "            while word[-1].isalnum() is False and word[-1] is not '_' and len(word)>1:\n",
    "                word = word[:-1]\n",
    "            if word in nameCounts:\n",
    "                nameCounts[word]+=1\n",
    "            elif len(word) > 1:\n",
    "                nameCounts.update({word:1})\n",
    "                \n",
    "sortD = sorted(nameCounts.items(),key=itemgetter(1))\n",
    "                \n",
    "print(\"The following list is the top 10 Twitter usernames mentioned:\")   \n",
    "for i in range(10):\n",
    "    print(sortD[-i-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Adapted from the solution to Q4; I search for # instead of @. However, inspection of the data reveals the # is sometimes actually used for its original purpose, to refer to numbers. \n",
    "\n",
    "The rules governing hashtags state that they must be alphanumeric and that they aren't case sensitive.\n",
    "\n",
    "I account for #'s not associated with hashtags by checking that the remainder of the string after # is alphanumeric. I also force the strings into lower case since hashtags are not case sensitive, so #Halloween and #halloween are considered the same hashtag.\n",
    "\n",
    "Finally, the algorithm accounts for punctuation after a hashtag, such as #halloween!!! in the same manner as before with usernames.\n",
    "\n",
    "This does not rule out the possibility of someone using it as an account number, like \"Here's my account #54345234\" but Twitter would likely interpret this as a hashtag anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 64865 unique Twitter hashtags in this file\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "nameCounts = {}\n",
    "for leString in data[4]:\n",
    "    splitString = leString.split()\n",
    "    for word in splitString:\n",
    "        if word[0] == '#' and len(word) > 1:\n",
    "            testword = word[1:]\n",
    "            while testword[-1].isalnum() is False and len(testword) > 1:\n",
    "                word = word[:-1]\n",
    "                testword = word\n",
    "            if testword.isalnum():\n",
    "                if word.lower() in nameCounts:\n",
    "                    nameCounts[word.lower()]+=1\n",
    "                else:\n",
    "                    nameCounts.update({word.lower():1})\n",
    "                \n",
    "print(\"There are\",len(nameCounts),\"unique Twitter hashtags in this file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: Adapted from the solution to questions 5 and 6; the difference is, again, that hashtags are not case sensitive and must be alphanumeric. I also create a sorted list and the last 10 entries are the most mentioned hashtags. As before, extra punctuation is stripped from the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following list is the top 10 Twitter hashtags mentioned:\n",
      "#fail\n",
      "#amazon\n",
      "#ios11\n",
      "#iphonex\n",
      "#customerservice\n",
      "#apple\n",
      "#help\n",
      "#aateam\n",
      "#hppsdr\n",
      "#iphone\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "nameCounts = {}\n",
    "for leString in data[4]:\n",
    "    splitString = leString.split()\n",
    "    for word in splitString:\n",
    "        if word[0] == '#' and len(word) > 1:\n",
    "            testword = word[1:]\n",
    "            while testword[-1].isalnum() is False and len(testword) > 1:\n",
    "                word = word[:-1]\n",
    "                testword = word\n",
    "            if testword.isalnum():\n",
    "                if word.lower() in nameCounts:\n",
    "                    nameCounts[word.lower()]+=1\n",
    "                else:\n",
    "                    nameCounts.update({word.lower():1})\n",
    "\n",
    "sortD = sorted(nameCounts.items(),key=itemgetter(1))\n",
    "                \n",
    "print(\"The following list is the top 10 Twitter hashtags mentioned:\")   \n",
    "for i in range(10):\n",
    "    print(sortD[-i-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8: Adapted from Q5 and Q7; I only count \"words\" that contain alphanumeric characters, purposely avoiding hashtags or usernames that start with # or @. I also force all strings into lower case to account for differences in case. Finally, I account for punctuation using the previous method. Plurals and possessive nouns are treated as separate words. I found that accounting for punctuation actually changes this list quite a lot, but that is reasonable given that many customer support tweets probably end similarly. \n",
    "\"Glad we could help!\" \"Glad we could help.\" \"glad we could help\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following list is the top 20 words mentioned in these Tweets:\n",
      "you\n",
      "help\n",
      "here\n",
      "this\n",
      "there\n",
      "it\n",
      "hi\n",
      "thanks\n",
      "us\n",
      "dm\n",
      "that\n",
      "out\n",
      "now\n",
      "number\n",
      "me\n",
      "hello\n",
      "issue\n",
      "today\n",
      "address\n",
      "account\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "nameCounts = {}\n",
    "for leString in data[4]:\n",
    "    splitString = leString.split()\n",
    "    for word in splitString:\n",
    "        while word[-1].isalnum() is False and len(word)>1:\n",
    "            word = word[:-1]\n",
    "            if word.lower() in nameCounts:\n",
    "                nameCounts[word.lower()]+=1\n",
    "            elif word.isalnum() is True:\n",
    "                nameCounts.update({word.lower():1})\n",
    "\n",
    "sortD = sorted(nameCounts.items(),key=itemgetter(1))\n",
    "                \n",
    "print(\"The following list is the top 20 words mentioned in these Tweets:\")   \n",
    "for i in range(20):\n",
    "    print(sortD[-i-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "Dictionaries are much faster than trying to search lists for names. I went from a program that took about 8 hours to run (before I gave up and killed it) to a program that takes about 2.5 minutes (on my desktop). Point taken!\n",
    "\n",
    "I suppose we shouldn't be surprised by the top 20 words.\n",
    "\n",
    "I imagine the quality control steps you take can vary the answers to some of these questions quite a bit. I suppose thats what distinguishes an experienced data scientist from an inexperienced one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
